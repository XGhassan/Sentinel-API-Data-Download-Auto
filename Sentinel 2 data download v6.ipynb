{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b030c1-128e-4e8d-a9fa-c08161dc329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732fe16-0798-4069-951b-0523eae8013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import wget\n",
    "import rasterio\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "import glob\n",
    "import zipfile\n",
    "import time\n",
    "#from datetime import datetime, timedelta\n",
    "import datetime\n",
    "\n",
    "import rioxarray\n",
    "from rasterio import plot\n",
    "from rasterio import mask\n",
    "from rasterio.mask import mask\n",
    "import xarray \n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import requests\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('max_colwidth', None)\n",
    "\n",
    "from osgeo import gdal, osr\n",
    "from rasterio.warp import reproject, Resampling\n",
    "\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Creating and plotting the flowchart libs:\n",
    "import pygraphviz as pgv\n",
    "from PIL import Image\n",
    "\n",
    "# Library used to get all folders in a given directory\n",
    "# It is used in the removal process\n",
    "import shutil\n",
    "\n",
    "# stop warning:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "# adding a background map\n",
    "import contextily as ctx\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d508641-267f-48d4-ad43-943f284f15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the area of interest\n",
    "aoi = gpd.read_file(\"./shapefiles/area_of_intrest.shp\")\n",
    "\n",
    "# Transform to WGS84\n",
    "aoi_wgs = aoi.to_crs('EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae9c3d-1b7f-451c-afd2-6ff6981cc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sentinel api\n",
    "api = SentinelAPI('UserName','Password','https://apihub.copernicus.eu/apihub/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc32534-186c-4cae-b683-22e0ecac40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a footprint of the data\n",
    "footprint = None\n",
    "for i in aoi_s_wgs['geometry']:\n",
    "    footprint = i\n",
    "\n",
    "\n",
    "# entering the required start date\n",
    "start_date = '2019-01-01'\n",
    "\n",
    "# entering the end date\n",
    "end_date = '2023-04-10'\n",
    "\n",
    "# changing the dates format\n",
    "api_start = start_date.translate( { ord(\"-\"): None } )\n",
    "api_end = end_date.translate( { ord(\"-\"): None } )\n",
    "\n",
    "# extrating the all available products:\n",
    "satellite_data = api.query(\n",
    "    footprint,\n",
    "    date =(api_start, api_end),\n",
    "    platformname = 'Sentinel-2',\n",
    "    processinglevel = 'Level-2A',\n",
    "    cloudcoverpercentage = (0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0473d-f7ff-4e4a-b324-1f1e2729e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convering the results to a geodataframe for further filtration \n",
    "products = api.to_geodataframe(satellite_data)\n",
    "\n",
    "# \n",
    "products_aoi = products[products['filename'].str.contains(\"R120_T40QEM\")]\n",
    "products_aoi = products_aoi[products_aoi['platformserialidentifier'] == 'Sentinel-2A']\n",
    "products_aoi['date'] = pd.to_datetime(products_aoi['beginposition']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9872f-d521-4042-a969-2ac3597e187d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extrating the products dates to a list\n",
    "dates_list = pd.to_datetime(products_aoi['beginposition']).dt.date.tolist()\n",
    "\n",
    "# using the input dates, generate dates that are 3 months apart:\n",
    "required_dates = pd.date_range(start_date,\n",
    "                    end_date,\n",
    "                    freq='10d').strftime('%Y-%m-%d').tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63492e75-d4a3-4bf9-8e18-3c9c585e550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the dates \n",
    "formulated_dates =[]\n",
    "for date in required_dates:\n",
    "    try:\n",
    "        read = datetime.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "        formulated_dates.append(read)\n",
    "    except ValueError:\n",
    "        print(\"ValueError in: \")\n",
    "        print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e4123-6fa1-4bed-82af-85eafbecd472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selecting the dates in the products df that are nearest to the dates of the entered dates.\n",
    "selected_dates = []\n",
    "for date in formulated_dates:\n",
    "    selected_dates.append(min(dates_list , key=lambda sub: abs(sub - date)))\n",
    "selected_dates = list(set(selected_dates))\n",
    "selected_dates.sort()\n",
    "selected_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b9f64-8ab8-4ca1-aa71-d2af84571ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracting the data based on the dates:\n",
    "scenes = products_aoi[products_aoi['date'].isin(selected_dates)].to_dict('records')\n",
    "scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf1c7a-894d-41c2-96d8-00309fb5c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a downloads file in the path, if it does not exist\n",
    "download_dir = os.path.join(os.getcwd() , 'downloads')\n",
    "os.makedirs(download_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5680cb-0806-4edd-88a9-7c105ed1ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les that are part of the current analysis:\n",
    "files_to_delete = [scene['title'] + ext for scene in scenes for ext in ['.zip', '.SAFE']]\n",
    "\n",
    "\n",
    "def delete_files_and_folders_not_in_list(file_list):    \n",
    "    # Get a list of all files and folders in the directory\n",
    "    all_items = os.listdir(download_dir)\n",
    "    \n",
    "    # Iterate over each item in the directory\n",
    "    for item_name in all_items:\n",
    "        # Create the item path\n",
    "        item_path = os.path.join(download_dir, item_name)\n",
    "        \n",
    "        # Check if the item is not in the provided list\n",
    "        if item_name not in file_list:\n",
    "            # Check if the item path is a file\n",
    "            if os.path.isfile(item_path):\n",
    "                # Delete the file\n",
    "                os.remove(item_path)\n",
    "                print(f\"Deleted file: {item_path}\")\n",
    "            elif os.path.isdir(item_path):\n",
    "                # Delete the folder and its contents\n",
    "                shutil.rmtree(item_path)\n",
    "                print(f\"Deleted folder: {item_path}\")\n",
    "\n",
    "\n",
    "# Call the function to delete files not in the list\n",
    "delete_files_and_folders_not_in_list(files_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e56c3-bad8-4157-bd66-a9c1fe4110b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each scene id and attempt to download, try twice before moving to the next one\n",
    "downloaded_basenames = [os.path.basename(downloaded_file) for downloaded_file in glob.glob(os.path.join(download_dir, '**', '*.zip'), recursive=True)]\n",
    "downloaded_scene_titles = {os.path.splitext(downloaded_basename)[0] for downloaded_basename in downloaded_basenames}\n",
    "\n",
    "all_scene_titles = {scene['title'] for scene in scenes}\n",
    "\n",
    "scene_ids = [scene['uuid'] for scene in scenes if scene['title'] in all_scene_titles.difference(downloaded_scene_titles)]\n",
    "# print(\"[INFO] {} scenes remain to download\".format(len(scene_ids)))\n",
    "\n",
    "\n",
    "for idx, scene_id in enumerate(scene_ids):\n",
    "    product_info = api.get_product_odata(scene_id)\n",
    "    print(\"[INFO] Product {} ({}/{}); Online: {}\".format(\n",
    "        product_info['id'],\n",
    "        idx + 1,\n",
    "        len(scene_ids),\n",
    "        product_info['Online']))\n",
    "    attempts = 0\n",
    "    \n",
    "    while attempts < 1:\n",
    "        try:\n",
    "            api.download(product_info['id'], download_dir)\n",
    "            break  # If successful, move past\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Request failed.. (current time: {})\".format(\n",
    "                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "        attempts += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# update the list after the initial request to select any scenes that were not downloaded\n",
    "downloaded_basenames = [os.path.basename(downloaded_file) for downloaded_file in glob.glob(os.path.join(download_dir, '**', '*.zip'), recursive=True)]\n",
    "downloaded_scene_titles = {os.path.splitext(downloaded_basename)[0] for downloaded_basename in downloaded_basenames}\n",
    "scene_ids = [scene['uuid'] for scene in scenes if scene['title'] in all_scene_titles.difference(downloaded_scene_titles)]\n",
    "\n",
    "# this loop wait 30 minutes after the initial downloading so that scenes become online \n",
    "if len(scene_ids) == 0:\n",
    "    print(\"[INFO] No more files left to download from CSV files\")\n",
    "else:\n",
    "    print(\"[INFO] {} scenes remain to download\".format(len(scene_ids)))\n",
    "    print('[INFO] Sleeping for 31 minutes until the files are online to download (downloading will resume at: {})'.format(\n",
    "                    (datetime.datetime.now() + datetime.timedelta(minutes=31)).strftime(\"%Y-%m-%d %H:%M\") ))\n",
    "    time.sleep(60 * 30)\n",
    "    for idx, scene_id in enumerate(scene_ids):\n",
    "        product_info = api.get_product_odata(scene_id)\n",
    "        print(\"[INFO] Product {} ({}/{}); Online: {}\".format(\n",
    "            product_info['id'],\n",
    "            idx + 1,\n",
    "            len(scene_ids),\n",
    "            product_info['Online']))\n",
    "        attempts = 0\n",
    "        \n",
    "        while attempts < 10:\n",
    "            try:\n",
    "                api.download(product_info['id'], download_dir)\n",
    "                break  # If successful, move past\n",
    "            except Exception as e:\n",
    "                wait_time = 2\n",
    "                print(\"[ERROR] Request failed.. waiting {} seconds before retrying (current time: {})\".format(\n",
    "                    wait_time * 60,\n",
    "                    datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "                time.sleep(60 * wait_time)\n",
    "            attempts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a760bd-f239-4838-8a64-c9c4b5bb9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all the downloade files:\n",
    "def un_zipFiles(download_dir):\n",
    "    files=os.listdir(download_dir)\n",
    "    for file in files:\n",
    "        if file.endswith('.zip'):\n",
    "            filePath=download_dir+'/'+file\n",
    "            zip_file = zipfile.ZipFile(filePath)\n",
    "            for names in zip_file.namelist():\n",
    "                zip_file.extract(names,download_dir)\n",
    "            zip_file.close() \n",
    "            \n",
    "un_zipFiles(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328b8de-87a7-4c7b-9190-4a8a2164deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_polygons = list(aoi.geometry)\n",
    "aoi_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe829e-cac5-4e8a-a37a-631e6230fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function import the band, and filter it to remove any saturated pixels\n",
    "def importing_jp2_before(item,list):\n",
    "    image_name = item.name.split('.')[0]\n",
    "    list.append(image_name)\n",
    "    xds = rioxarray.open_rasterio(os.path.join(download_dir, item))\n",
    "    xds = xds.where(xds > 1, other = 1)\n",
    "    xds = xds.where(xds < 10000, other = 1)\n",
    "    xds = xds.rio.clip(aoi_polygons, aoi.crs, drop=True).to_dataset(name ='band_data')\n",
    "    globals()[image_name] = xds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1d07f-d478-4d7b-ab17-700d5151ac75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Band3_list = []   # Green\n",
    "Band4_list = []   # Red\n",
    "Band8_list = []   # NIR\n",
    "\n",
    "# importing the required scenes and adding their names to the lists above\n",
    "for item in Path(download_dir).glob('**/*10m.jp2'):\n",
    "    image_date = item.name.split('_')[1].split('T')[0]\n",
    "    image_date = datetime.datetime.strptime(image_date, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    if item.name.endswith('B03_10m.jp2'):\n",
    "        importing_jp2_before(item,Band3_list)\n",
    "    if item.name.endswith('B04_10m.jp2'):\n",
    "        importing_jp2_before(item,Band4_list)\n",
    "    if item.name.endswith('B08_10m.jp2'):\n",
    "        importing_jp2_before(item,Band8_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefcfc5-009a-460e-86b9-407d6a9461e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si_9 = (nir*red)/green\n",
    "\n",
    "# generating the soil salinity index 9\n",
    "si_9_result = []\n",
    "for index, band in enumerate(selected_dates):\n",
    "    date_name = format(band.strftime('%Y%m%d'))\n",
    "    file_name = \"si_9_{}\".format(date_name)\n",
    "    band_4 = locals()[[i for i in vars() if date_name in i and 'B04' in i][0]]\n",
    "    band_8 = locals()[[i for i in vars() if date_name in i and 'B08' in i][0]] \n",
    "    band_3 = locals()[[i for i in vars() if date_name in i and 'B03' in i][0]]\n",
    "    if selected_dates[index] < datetime.datetime.strptime('2022-01-25', '%Y-%m-%d').date():\n",
    "        results = ((band_4) * (band_8)) / (band_3)\n",
    "    elif selected_dates[index] >= datetime.datetime.strptime('2022-01-25', '%Y-%m-%d').date():\n",
    "        band_4_e = band_4 - 1000\n",
    "        band_8_e = band_8 - 1000\n",
    "        band_3_e = band_3 - 1000\n",
    "        band_4_e = band_4_e.where(band_4_e['band_data'] > 1, other = 1)\n",
    "        band_8_e = band_8_e.where(band_8_e['band_data'] > 1, other = 1)\n",
    "        band_3_e = band_3_e.where(band_3_e['band_data'] > 1, other = 1)\n",
    "        results= ((band_4_e * band_8_e) / band_3_e)\n",
    "    results = results.where(results > 1, other = 1)\n",
    "    results = results.where(results < 500, other = 1)\n",
    "    results = results.rio.clip(aoi_polygons, aoi.crs, drop=True)\n",
    "    print(results.band_data.mean().item())\n",
    "    results[\"time\"] = np.datetime64(band)\n",
    "    results = results.rename_vars({'band_data': 'index_9'})\n",
    "    globals()[file_name] = results\n",
    "    si_9_result.append(vars()[f\"{file_name}\"])\n",
    "    results.to_netcdf('C:/Users/G/Desktop/Diss/soil_salinity/full_resolution/' + file_name + '.nc4', engine=\"netcdf4\", encoding = {\"index_9\": {\"dtype\": \"float64\"}})\n",
    "\n",
    "\n",
    "si_9_results = xarray.concat(si_9_result, dim=\"time\")\n",
    "si_9_results.to_netcdf('C:/Users/G/Desktop/Diss/soil_salinity/ss_9.nc4', engine=\"netcdf4\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9da93c-ebae-4b7d-9d10-1e08ef487778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generating the soil salinity index 9\n",
    "\n",
    "NDVI_result = []\n",
    "for band in selected_dates:\n",
    "    date_name = format(band.strftime('%Y%m%d'))\n",
    "    file_name = \"ndvi_{}\".format(date_name)\n",
    "    band_4 = locals()[[i for i in vars() if date_name in i and 'B04' in i][0]]\n",
    "    band_8 = locals()[[i for i in vars() if date_name in i and 'B08' in i][0]] \n",
    "    if selected_dates[index] < datetime.datetime.strptime('2022-01-25', '%Y-%m-%d').date():\n",
    "        results = ((band_8) - (band_4)) / ((band_8) + (band_4))\n",
    "    elif selected_dates[index] >= datetime.datetime.strptime('2022-01-25', '%Y-%m-%d').date():\n",
    "        band_4_mean = band_4.band_data.mean().item()\n",
    "        band_8_mean = band_8.band_data.mean().item()\n",
    "        band_4_e = band_4 - 1000\n",
    "        band_8_e = band_8 - 1000\n",
    "        band_4_e = band_4_e.where(band_4_e['band_data'] > 1, other = 1)\n",
    "        band_8_e = band_8_e.where(band_8_e['band_data'] > 1, other = 1)\n",
    "        results = (band_8 - band_4 ) / (band_8  + band_4)\n",
    "    results = results.where(results >= -1, other = 0)\n",
    "    results = results.where(results <= 1, other = 0)\n",
    "    results = results.rio.clip(aoi_polygons, aoi.crs, drop=True)\n",
    "    print(results.band_data.mean().item())\n",
    "    results = results.rename_vars({'band_data': 'ndvi'})\n",
    "    results[\"time\"] = np.datetime64(band)\n",
    "    globals()[file_name] = results\n",
    "    results.to_netcdf('C:/Users/G/Desktop/Diss/NDVI/full_resolution/' + file_name + '.nc4', engine=\"netcdf4\")\n",
    "    NDVI_result.append(vars()[f\"{file_name}\"])\n",
    "\n",
    "\n",
    "\n",
    "NDVI_results = xarray.concat(NDVI_result, dim=\"time\")\n",
    "NDVI_results.to_netcdf('C:/Users/G/Desktop/Diss/NDVI/NDVI.nc4', engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcc634-9633-49f1-85ae-cdfb23a03b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyKer",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
